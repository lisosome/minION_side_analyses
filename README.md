# minION_side_analyses

A little workflow to perform mithocondrial and *STRC* variant calling from long range PCR sequenced with minION. It is based on SLURM job dependancy system, so the workflow **MUST** be executed in the ORFEO cluster. 

The main script "minION_analyses.sh" can be executed directly or submitting a job to the SLURM queue manager:
    1. Direct execution:
```bash
            ./minION_analyses.sh \
            -r {raw_data_folder} \
            -s {samplesheet} \
            -o {output_directory} \
            -m {execution_mode}

```
    2. Job submission:
```bash
            sbatch -A burlo -o {log_file} -e {err_file} \
            ./minION_analyses.sh \
                -r {raw_data_folder} \
                -s {samplesheet} \
                -o {output_directory} \
                -m {execution_mode}
```

The flag to be specified are:
    - **-r:** Directory storing the minION-produced raw data. The one that must be uploaded from the minION to Orfeo in order to run the analyses
    - **-s:** Space-separated samplesheet storing the DNA code of the sample in the first column; the barcode in the second column
    - **-o:** Output directory in which store the analyses' results
    - **-m:** Modality of execution. Choose MT for mithocondrial variant calling; STRC for variant calling of variants in the STRC gene

The pipeline uses several script stored in the *src/* folder to perform the analyses. This folder is divided in three subfolders:
    **preprocessing:** Storing script for basecalling, trimming and aligning or minION data
    **strc:** Storing the script to perform the variant calling for STRC gene using *clair3* [https://github.com/HKU-BAL/Clair3]
    **mtdna** Storing the script to perform the variant calling for mithocondrial DNA as performed by *Mitoverse* [https://mitoverse.i-med.ac.at/#!]

## Resources

Resources needed to run the minipipe;ine are hardcoded in the scripts and are stored in /fast/burlo/nardone. 
**WARNING:** if permission problems arise, contact me at [giuseppegiovanni.nardone@burlo.trieste.it] or copy the resources to a local folder, and modify permissions using `chmod 777`.

## Execution

To execute the pipeline first download the code using git:
    
```bash
        git clone https://github.com/lisosome/minION_side_analyses.git
```

Then, to avoid execution problems, abilitate the execution of every script using `chmod +x`:

```bash
        find minION_side_analyses -name "*.sh" -exec chmod +x {} \;
```

Finally, execute the main script to launch all the jobs.


Once completed, the workflow will create different folders containing different files generated from the different analysis steps. The folder structure is identical to the one produced by the minION_snakemake pipeline. Check its *wiki* [https://github.com/lisosome/minION_snakemake/wiki/Whole-Genome-Sequencing-analysis-protocol] for more information.
The only exception is the **4.VARIANT.CALLING** folder generated by the mithocondrial variant calling. This will be divided in:
    
```
    4.VARIANT.CALLING/
            ├── annotation
            │   └── {sample_code}.mtDNA.variants.annotated.txt # annotated mithocondrial variants
            ├── filtering
            │   ├── {sample_code}.mutect2.filtered.txt # Filtered indels called by mutect2
            │   ├── {sample_code}.mutserve.filtered.txt # Filtered variants called by mutserve
            │   └── work # Directory storing intermediate results of the filtering step
            │       ├── {sample_code}.mutect2.tmp.txt
            │       ├── {sample_code}.mutect2.txt
            │       ├── {sample_code}.mutserve.tmp.txt
            │       └── {sample_code}.mutserve.txt
            ├── merging 
            │   ├── {sample_code}.mtDNA.variants.txt # merged SNVs and InDels to be annotated
            │   └── work # Directory storing intermediate results of the merging step
            │       ├── {sample_code}.variants.concat.txt
            │       └── {sample_code}.variants.sorted.txt
            ├── mutect2 
            │   ├── {sample_code}.norm.mutect2.vcf.gz # VCF file containing indels called by mutect2
            │   ├── {sample_code}.norm.mutect2.vcf.gz.tbi # VCF index file containing indels called by mutect2
            │   └── work # Directory storing intermediate results of mutect2 variant calling 
            │       ├── {sample_code}.mutect2.raw.filtered.vcf.gz
            │       ├── {sample_code}.mutect2.raw.filtered.vcf.gz.filteringStats.tsv
            │       ├── {sample_code}.mutect2.raw.filtered.vcf.gz.tbi
            │       ├── {sample_code}.mutect2.raw.vcf.gz
            │       ├── {sample_code}.mutect2.raw.vcf.gz.stats
            │       └── {sample_code}.mutect2.raw.vcf.gz.tbi
            ├── mutserve
            │   ├── {sample_code}.norm.mutserve.vcf.gz # VCF file containing indels called by mutserve
            │   ├── {sample_code}.norm.mutserve.vcf.gz.tbi # VCF index file containing indels called by mutserve
            │   └── work # Directory storing intermediate results of mutserve variant calling
            │       ├── {sample_code}_raw.txt
            │       ├── {sample_code}.tmp.mutserve.vcf.gz
            │       ├── {sample_code}.tmp.norm.mutserve.vcf.gz
            │       ├── {sample_code}.tmp.norm.mutserve.vcf.gz.tbi
            │       ├── {sample_code}.txt
            │       └── sample_renaming.tsv
            └── vcf_merge
                ├── {sample_code}.filt.SNPs.INDELs.vcf.gz # mutect2 and mutserve merged VCF
                ├── {sample_code}.filt.SNPs.INDELs.vcf.gz.tbi # mutect2 and mutserve merged VCF index
                └── work # Directory storing intermediate results of VCF merging
                    ├── {sample_code}.mutect2.indels.vcf.gz
                    ├── {sample_code}.mutect2.indels.vcf.gz.tbi
                    ├── {sample_code}.SNPs_INDELs.vcf.gz
                    ├── {sample_code}.SNPs_INDELs.vcf.gz.tbi
                    └── pos_file.tsv
```

## Known Issues

Not quite a proper issue, but the execution of this analyses is unidirectional. It does **NOT** have a caching system of any sort, so if a problem occurs during the execution of the workflow, launching the main script will restart the processes from the beginning. If you want to resume from a specific step of the workflow, use the scripts stored in the src folder.